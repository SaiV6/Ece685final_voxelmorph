# -*- coding: utf-8 -*-
"""UNET.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r-sLaSVWA_prn0nY-IR1w5nCelE831-I

# IMPORT DATASET

**OASIS Dataset import**
"""

# # get neurite-OASIS data
# !wget https://surfer.nmr.mgh.harvard.edu/ftp/data/neurite/data/neurite-oasis.v1.0.tar
# !mkdir OASIS
# !tar xf neurite-oasis.v1.0.tar --directory 'OASIS'

# from google.colab import drive
# drive.mount('/content/drive')

#Import necessary libraries
import os
import random
import argparse
import time
import numpy as np
import torch
import torch.nn as nn
import torch.optim
import matplotlib.pyplot as plt
import torchvision.datasets as datasets
import torch.nn.functional as F
from torch.distributions.normal import Normal
import os
import torch
from torch.utils.data import Dataset, DataLoader
import nibabel as nib
from torchvision import transforms

"""# NEURAL NETWORK

**Define the Neural Network**
"""

# Define the neural network using Voxelmorph Architecture
class VoxelMorph(nn.Module):
    def __init__(self,in_channels=2, maxpool = False, reduce_size = False):
        # Input is [batch size x 2 x X x Y x Z], where input[i] is the i-th m and f concatenated together into a 2 channel 3d image
        super().__init__()
        self.maxpool = maxpool
        self.reduce_size = reduce_size

        self.act = nn.LeakyReLU(0.2)
        self.up = nn.Upsample(scale_factor = 2)
        self.max = nn.MaxPool3d(2, stride=2)
        #Encoder: Conv3d + LeakyReLU Layers

        # Maxpool or downsample with Conv?
        self.econv1 = nn.Conv3d(2, 16, 3, stride = 1, padding = 1) # Output size is same, 16 channels
        self.econv2 = nn.Conv3d(16, 32, 3, stride = 2, padding = 1) # Output size is 1/2, 32 channels
        self.econvX = nn.Conv3d(32, 32, 3, stride = 2, padding = 1)

        #Encoder END
        #Decoder Conv3d + LeakyReLU + Upsample + UNET Concat

        self.dconv1 = nn.Conv3d(32, 32, 3, stride = 1, padding = 1) # Output size is 1/16, 32 channels
        self.dconvX = nn.Conv3d(64, 32, 3, stride = 1, padding = 1)
        self.dconv5 = nn.Conv3d(48, 16, 3, stride = 1, padding = 1) # Output size is 1, 16 channels
        self.dconv6 = nn.Conv3d(16, 16, 3, stride = 1, padding = 1) # Output size is 1, 16 channels
        self.dconv7 = nn.Conv3d(16, 3, 3, stride = 1, padding = 1) # Output size is 1, 3 channels

    def forward(self, x):

        if self.maxpool:
          x = self.max(x)
        # Encoder

        ex1 = self.act(self.econv1(x))
        ex2 = self.act(self.econv2(ex1))
        ex3 = self.act(self.econvX(ex2))
        if not self.reduce_size:
          ex4 = self.act(self.econvX(ex3))
          ex5 = self.act(self.econvX(ex4))

        # Decoder
        if not self.reduce_size:
          dx = self.up(self.act(self.dconv1(ex5)))
          dx = torch.cat((dx, ex4), dim = 1)
          dx = self.up(self.act(self.dconvX(dx)))
          dx = torch.cat((dx, ex3), dim = 1)
          dx = self.up(self.act(self.dconvX(dx)))
          dx = torch.cat((dx, ex2), dim = 1)
          dx = self.up(self.act(self.dconvX(dx)))
          dx = torch.cat((dx, ex1), dim = 1)
          dx = self.act(self.dconv5(dx))
          dx = self.act(self.dconv6(dx))
          dx = self.act(self.dconv7(dx))
        else:
          dx = self.up(self.act(self.dconv1(ex3)))
          dx = torch.cat((dx, ex2), dim = 1)
          dx = self.up(self.act(self.dconvX(dx)))
          dx = torch.cat((dx, ex1), dim = 1)
          dx = self.act(self.dconv5(dx))
          dx = self.act(self.dconv6(dx))
          dx = self.act(self.dconv7(dx))

        if self.maxpool:
          dx = self.up(dx)

        return dx

start = time.time()
test = VoxelMorph(maxpool = True, reduce_size = True).cuda()
tempin = torch.zeros((1, 2, 160, 192, 224)).cuda()
out = test(tempin)
print(time.time() - start)

# the 'superclass' of VoxelMorph. It calls VoxelMorph, SpatialTransformer, and contains trainable params
class VxmDense(nn.Module):
    def __init__(self, in_channels, inshape):
        super().__init__()
        self.in_channels = in_channels

        # initialize the U-net
        self.unet_model = VoxelMorph(
            in_channels=self.in_channels,
            maxpool = False, reduce_size = True
        )
        ndims = len(inshape)
        assert ndims in [1, 2, 3], 'ndims should be one of 1, 2, or 3. found: %d' % ndims

        # the flow field
        Conv = getattr(nn, 'Conv%dd' % ndims)
        self.flow = Conv(3, ndims, kernel_size=3, padding=1)

        # params to be trained
        self.flow.weight = nn.Parameter(Normal(0, 1e-5).sample(self.flow.weight.shape))
        self.flow.bias = nn.Parameter(torch.zeros(self.flow.bias.shape))

        # initialize the spatial transformer
        self.transformer = SpatialTransformer(inshape)

    def forward(self, source, target):
        '''
        Parameters:
            source: the moving image
            target: the reference image
        '''
        # concatenate inputs and propagate unet
        x = torch.cat([source, target], dim=1)
        x = self.unet_model(x)

        # transform into flow field
        flow_field = self.flow(x)

        pos_flow = flow_field

        # warp image with flow field
        y_source = self.transformer(source, pos_flow)
        return y_source, pos_flow

# the spatial transformer warps the moving image based on the flow field
class SpatialTransformer(nn.Module):
    def __init__(self, inshape):
        super().__init__()

        size = inshape
        vectors = [torch.arange(0, s) for s in size]
        grids = torch.meshgrid(vectors)
        grid = torch.stack(grids)
        grid = torch.unsqueeze(grid, 0)
        grid = grid.type(torch.FloatTensor)
        self.register_buffer('grid', grid)

    def forward(self, src, flow):
        # new locations
        new_locs = self.grid + flow
        shape = flow.shape[2:]

        # need to normalize grid values to [-1, 1] for resampler
        for i in range(len(shape)):
            new_locs[:, i, ...] = 2 * (new_locs[:, i, ...] / (shape[i] - 1) - 0.5)

        # move channels dim to last position
        if len(shape) == 2:
            new_locs = new_locs.permute(0, 2, 3, 1)
            new_locs = new_locs[..., [1, 0]]
        elif len(shape) == 3:
            new_locs = new_locs.permute(0, 2, 3, 4, 1)
            new_locs = new_locs[..., [2, 1, 0]]

        return F.grid_sample(src, new_locs, align_corners=True, mode='bilinear')

"""# Data Loader

**Create the Train Loader**
"""

class CustomDataset(Dataset):
    def __init__(self, root_dir):
        # Initialize the CustomDataset with the root directory
        self.root_dir = root_dir
        # Create a list of patient names by filtering only directories in the root directory
        self.patient_list = [f for f in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, f))]

    def __len__(self):
        # Return the total number of patients in the dataset
        return len(self.patient_list)

    def __getitem__(self, idx):
        # Get the name of the patient at the specified index
        patient_name = self.patient_list[idx]

        # Construct file paths for the aligned original and normalized images for the current patient
        aligned_orig_path = os.path.join(self.root_dir, patient_name, 'aligned_orig.nii.gz')
        aligned_norm_path = os.path.join(self.root_dir, patient_name, 'aligned_norm.nii.gz')

        # Load aligned_orig.nii.gz
        aligned_orig = nib.load(aligned_orig_path).get_fdata()

        # Load aligned_norm.nii.gz
        aligned_norm = nib.load(aligned_norm_path).get_fdata()

        # Apply any additional transformations if needed
        transform = transforms.Compose([
            # Add your transformations here if needed
        ])

        # Apply transformations
        aligned_orig = transform(aligned_orig)
        aligned_norm = transform(aligned_norm)

        # Combine the images along a new axis
        stacked_images = torch.stack([torch.from_numpy(aligned_orig), torch.from_numpy(aligned_norm)], dim=0)

        return stacked_images

#set up the location for the OASIS dataset
root_directory = '/content/OASIS'
custom_dataset = CustomDataset(root_dir=root_directory)
data_loader = DataLoader(custom_dataset, batch_size=8, shuffle=False)

# Iterate through the data loader
for batch in data_loader:
    image_batch = batch
    print(image_batch.shape)  # Shape should be (batch_size, num_channels, height, width, depth)
    break  # Break after processing the first batch

"""# LOSS

**Define the Loss Functions**
"""

import math

# The MSE loss
def MSE(y_true, y_pred):
    return torch.mean((y_true - y_pred) ** 2)

# The negative normalized cross-correlation
def NCC(y_true, y_pred, win=None):
    Ii = y_true
    Ji = y_pred

    # get dimension of volume
    # assumes Ii, Ji are sized [batch_size, *vol_shape, nb_feats]
    ndims = len(list(Ii.size())) - 2
    assert ndims in [1, 2, 3], "volumes should be 1 to 3 dimensions. found: %d" % ndims

    # set window size
    win = [9] * ndims if win is None else win

    # compute filters
    sum_filt = torch.ones([1, 1, *win]).to("cuda")

    pad_no = math.floor(win[0] / 2)

    if ndims == 1:
        stride = (1)
        padding = (pad_no)
    elif ndims == 2:
        stride = (1, 1)
        padding = (pad_no, pad_no)
    else:
        stride = (1, 1, 1)
        padding = (pad_no, pad_no, pad_no)

    # get convolution function
    conv_fn = getattr(F, 'conv%dd' % ndims)

    # compute CC squares
    I2 = Ii * Ii
    J2 = Ji * Ji
    IJ = Ii * Ji

    I_sum = conv_fn(Ii, sum_filt, stride=stride, padding=padding)
    J_sum = conv_fn(Ji, sum_filt, stride=stride, padding=padding)
    I2_sum = conv_fn(I2, sum_filt, stride=stride, padding=padding)
    J2_sum = conv_fn(J2, sum_filt, stride=stride, padding=padding)
    IJ_sum = conv_fn(IJ, sum_filt, stride=stride, padding=padding)

    win_size = np.prod(win)
    u_I = I_sum / win_size
    u_J = J_sum / win_size

    cross = IJ_sum - u_J * I_sum - u_I * J_sum + u_I * u_J * win_size
    I_var = I2_sum - 2 * u_I * I_sum + u_I * u_I * win_size
    J_var = J2_sum - 2 * u_J * J_sum + u_J * u_J * win_size

    cc = cross * cross / (I_var * J_var + 1e-5)

    return -torch.mean(cc)

# calculate the finite difference. This is part of the regularizer
def diffs(y):
    vol_shape = [n for n in y.shape][2:]
    ndims = len(vol_shape)

    df = [None] * ndims
    for i in range(ndims):
        d = i + 2
        # permute dimensions
        r = [d, *range(0, d), *range(d + 1, ndims + 2)]
        y = y.permute(r)
        dfi = y[1:, ...] - y[:-1, ...]

        # permute back
        # note: this might not be necessary for this loss specifically,
        # since the results are just summed over anyway.
        r = [*range(d - 1, d + 1), *reversed(range(1, d - 1)), 0, *range(d + 1, ndims + 2)]
        df[i] = dfi.permute(r)

    return df

# the gradient regularizer
def smooth_grad(y_pred, penalty='l1', loss_mult=None):
    if penalty == 'l1':
        dif = [torch.abs(f) for f in diffs(y_pred)]
    else:
        assert self.penalty == 'l2', 'penalty can only be l1 or l2. Got: %s' % penalty
        dif = [f * f for f in diffs(y_pred)]

    df = [torch.mean(torch.flatten(f, start_dim=1), dim=-1) for f in dif]
    grad = sum(df) / len(df)

    if loss_mult is not None:
        grad *= loss_mult

    return grad.mean()

"""# TRAIN

**Set up the Training Loop**
"""

# Training Loop Setup

def make_optimizer(optimizer_name, model, **kwargs):
    if optimizer_name=='Adam':
        optimizer = torch.optim.Adam(model.parameters(),lr=kwargs['lr'])
    elif optimizer_name=='SGD':
        optimizer = torch.optim.SGD(model.parameters(),lr=kwargs['lr'],momentum=kwargs['momentum'], weight_decay=kwargs['weight_decay'])
    else:
        raise ValueError('Not valid optimizer name')
    return optimizer

def make_scheduler(scheduler_name, optimizer, **kwargs):
    if scheduler_name=='MultiStepLR':
        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=kwargs['milestones'],gamma=kwargs['factor'])
    else:
        raise ValueError('Not valid scheduler name')
    return scheduler

# train function
lossArray = []
def train(model, iterator, optimizer, epoch):
    global lossArray
    train_loss = 0
    correct = 0
    model.train()
    for i, imgs in enumerate(iterator):
      optimizer.zero_grad()
      img_fixed = imgs[:, 0:1, ...].to('cuda').to(torch.float)
      img_m = imgs[:, 1:2, ...].to('cuda').to(torch.float)
      output, _ = model(img_m, img_fixed)

      NCCloss = NCC(img_fixed, output)
      smooth_loss = smooth_grad(output)
      loss = NCCloss + smooth_loss
      loss.backward()

      optimizer.step()
      train_loss += loss.item()
      lossArray.append(loss.item())
      print('Loss:',loss.item())

device = 'cuda'

# initialize the model
model = VxmDense(in_channels=2, inshape=[160, 192, 224]).to(device)

# hyperparams
num_epochs = 10
learning_rate = 0.001
optimizer_name = 'Adam'
scheduler_name = 'MultiStepLR'

optimizer = make_optimizer(optimizer_name, model, lr=learning_rate, momentum=0, weight_decay=0)
scheduler = make_scheduler(scheduler_name, optimizer, milestones=[5], factor=0.1)

print('Start training...')
start = time.time()

# main loop
for epoch in range(1, num_epochs + 1):
    train(model, data_loader, optimizer, criterion, epoch)
    #scheduler.step()
    print('Epoch [{}/{}], Optimizer Learning rate: {:.4f}'.format(epoch, num_epochs, optimizer.param_groups[0]['lr']))
print('Done!')
print(time.time() - start)

plt.figure(0)
plt.plot(lossArray)
plt.xlabel("Training Step")
plt.ylabel("Loss")
plt.title("Training Loss")

"""# PLOTTING

**Plot the output, original moved, and original fixed images from 4 random slices**
"""

import matplotlib.pyplot as plt

# Create a 3x4 subplot grid for displaying reconstructed brain scans
fig, axs = plt.subplots(3, 4, figsize=(18, 12))
fig.suptitle('Voxelmorph-Reconstructed Brain Scans', fontsize=30)

# Iterate over batches of data in the data loader
for i, imgs in enumerate(data_loader):
    # Ensure no gradient computation during evaluation
    with torch.no_grad():
        # Set the model to evaluation mode
        model.eval()
        # Zero the gradients in the optimizer (not needed during evaluation)
        optimizer.zero_grad()

        # Extract fixed and moving images from the batch and move them to GPU
        img_fixed = imgs[:, 0:1, ...].to('cuda').to(torch.float)
        img_m = imgs[:, 1:2, ...].to('cuda').to(torch.float)

        # Perform inference with the model
        output, _ = model(img_m, img_fixed)

        # Extract slices from tensors and convert to NumPy arrays
        fixed_slice = img_fixed[0, 0, :, :, 112].cpu().detach().numpy()
        warped_slice = output[0, 0, :, :, 112].cpu().detach().numpy()
        original_slice = img_m[0, 0, :, :, 112].cpu().detach().numpy()

        # Display the fixed, deformed, and original slices in the subplot grid
        axs[0, i].imshow(fixed_slice, cmap='gray')
        axs[0, i].set_title('Fixed Slice')
        axs[0, i].get_xaxis().set_visible(False)
        axs[0, i].get_yaxis().set_visible(False)
        axs[1, i].imshow(warped_slice, cmap='gray')
        axs[1, i].set_title('Deformed Moving Slice')
        axs[1, i].get_xaxis().set_visible(False)
        axs[1, i].get_yaxis().set_visible(False)
        axs[2, i].imshow(original_slice, cmap='gray')
        axs[2, i].set_title('Original Moving Slice')
        axs[2, i].get_xaxis().set_visible(False)
        axs[2, i].get_yaxis().set_visible(False)

    # Break the loop after displaying the first 3 batches of data
    if i > 2:
        break